{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b5a5b0",
   "metadata": {},
   "source": [
    "# Sales Data Generator for Workshop\n",
    "\n",
    "This notebook generates realistic sales data for the **Fabric for the Power BI User** workshop.\n",
    "\n",
    "**Run this in:** Microsoft Fabric Lakehouse environment\n",
    "\n",
    "**What it creates:**\n",
    "- `Products` - 65 outdoor/sporting goods products\n",
    "- `Stores` - 12 retail locations across Pacific Northwest\n",
    "- `Calendar` - Date dimension (2023-2025)\n",
    "- `Targets` - Monthly revenue targets by store\n",
    "- `Sales` - Transaction-level sales data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cccb3d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings as needed for your demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40eb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NUM_TRANSACTIONS = 100000  # Number of sales transactions to generate\n",
    "START_DATE = \"2023-01-01\"\n",
    "END_DATE = \"2025-12-31\"\n",
    "\n",
    "# Set to True to append new data (for incremental demo)\n",
    "APPEND_MODE = False\n",
    "\n",
    "# Number of new transactions to add in append mode\n",
    "APPEND_COUNT = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea38df",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53123fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Spark session (available by default in Fabric)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Spark session ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21031b",
   "metadata": {},
   "source": [
    "## 1. Products Table\n",
    "\n",
    "65 products across 8 categories of outdoor and sporting goods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91184d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data = [\n",
    "    (\"1001\", \"Trail Runner Pro\", \"Footwear\", \"Running Shoes\", 129.99, 52.00, True),\n",
    "    (\"1002\", \"Summit Hiker\", \"Footwear\", \"Hiking Boots\", 189.99, 76.00, True),\n",
    "    (\"1003\", \"Urban Walker\", \"Footwear\", \"Casual Shoes\", 79.99, 32.00, True),\n",
    "    (\"1004\", \"Grip Master Sandal\", \"Footwear\", \"Sandals\", 59.99, 24.00, True),\n",
    "    (\"1005\", \"Winter Trek Boot\", \"Footwear\", \"Winter Boots\", 219.99, 88.00, True),\n",
    "    (\"1006\", \"Sprint Elite\", \"Footwear\", \"Running Shoes\", 149.99, 60.00, True),\n",
    "    (\"1007\", \"All-Terrain Mid\", \"Footwear\", \"Hiking Boots\", 159.99, 64.00, True),\n",
    "    (\"1008\", \"Classic Canvas\", \"Footwear\", \"Casual Shoes\", 49.99, 20.00, True),\n",
    "    (\"2001\", \"Ultralight Tent 2P\", \"Camping\", \"Tents\", 349.99, 140.00, True),\n",
    "    (\"2002\", \"Family Dome 6P\", \"Camping\", \"Tents\", 449.99, 180.00, True),\n",
    "    (\"2003\", \"Compact Sleep Bag\", \"Camping\", \"Sleeping Bags\", 129.99, 52.00, True),\n",
    "    (\"2004\", \"Arctic Sleep Bag\", \"Camping\", \"Sleeping Bags\", 249.99, 100.00, True),\n",
    "    (\"2005\", \"Quick Setup Canopy\", \"Camping\", \"Shelters\", 199.99, 80.00, True),\n",
    "    (\"2006\", \"Trekking Poles Set\", \"Camping\", \"Accessories\", 79.99, 32.00, True),\n",
    "    (\"2007\", \"LED Headlamp Pro\", \"Camping\", \"Lighting\", 44.99, 18.00, True),\n",
    "    (\"2008\", \"Camp Chair Deluxe\", \"Camping\", \"Furniture\", 89.99, 36.00, True),\n",
    "    (\"2009\", \"Portable Stove\", \"Camping\", \"Cooking\", 69.99, 28.00, True),\n",
    "    (\"2010\", \"Water Filter System\", \"Camping\", \"Hydration\", 49.99, 20.00, True),\n",
    "    (\"3001\", \"Performance Tee\", \"Apparel\", \"Tops\", 39.99, 16.00, True),\n",
    "    (\"3002\", \"Fleece Pullover\", \"Apparel\", \"Tops\", 79.99, 32.00, True),\n",
    "    (\"3003\", \"Rain Shell Jacket\", \"Apparel\", \"Outerwear\", 149.99, 60.00, True),\n",
    "    (\"3004\", \"Down Insulated Jacket\", \"Apparel\", \"Outerwear\", 229.99, 92.00, True),\n",
    "    (\"3005\", \"Hiking Pants\", \"Apparel\", \"Bottoms\", 69.99, 28.00, True),\n",
    "    (\"3006\", \"Quick-Dry Shorts\", \"Apparel\", \"Bottoms\", 49.99, 20.00, True),\n",
    "    (\"3007\", \"Merino Base Layer\", \"Apparel\", \"Base Layers\", 89.99, 36.00, True),\n",
    "    (\"3008\", \"Sport Socks 3-Pack\", \"Apparel\", \"Accessories\", 24.99, 10.00, True),\n",
    "    (\"3009\", \"Sun Hat Wide Brim\", \"Apparel\", \"Accessories\", 34.99, 14.00, True),\n",
    "    (\"3010\", \"Winter Beanie\", \"Apparel\", \"Accessories\", 29.99, 12.00, True),\n",
    "    (\"4001\", \"Mountain Bike 27.5\", \"Cycling\", \"Bikes\", 899.99, 450.00, True),\n",
    "    (\"4002\", \"Road Bike Carbon\", \"Cycling\", \"Bikes\", 1499.99, 750.00, True),\n",
    "    (\"4003\", \"Kids Bike 20in\", \"Cycling\", \"Bikes\", 299.99, 150.00, True),\n",
    "    (\"4004\", \"Bike Helmet Adult\", \"Cycling\", \"Safety\", 79.99, 32.00, True),\n",
    "    (\"4005\", \"Bike Lock Heavy Duty\", \"Cycling\", \"Accessories\", 49.99, 20.00, True),\n",
    "    (\"4006\", \"Cycling Gloves\", \"Cycling\", \"Apparel\", 34.99, 14.00, True),\n",
    "    (\"4007\", \"Bike Repair Kit\", \"Cycling\", \"Tools\", 29.99, 12.00, True),\n",
    "    (\"4008\", \"Water Bottle Cage\", \"Cycling\", \"Accessories\", 14.99, 6.00, True),\n",
    "    (\"5001\", \"Kayak Single 10ft\", \"Water Sports\", \"Kayaks\", 649.99, 260.00, True),\n",
    "    (\"5002\", \"Kayak Tandem 12ft\", \"Water Sports\", \"Kayaks\", 899.99, 360.00, True),\n",
    "    (\"5003\", \"Paddle Adjustable\", \"Water Sports\", \"Paddles\", 89.99, 36.00, True),\n",
    "    (\"5004\", \"Life Vest Adult\", \"Water Sports\", \"Safety\", 69.99, 28.00, True),\n",
    "    (\"5005\", \"Dry Bag 20L\", \"Water Sports\", \"Storage\", 39.99, 16.00, True),\n",
    "    (\"5006\", \"Snorkel Set\", \"Water Sports\", \"Snorkeling\", 49.99, 20.00, True),\n",
    "    (\"5007\", \"Beach Towel XL\", \"Water Sports\", \"Accessories\", 29.99, 12.00, True),\n",
    "    (\"6001\", \"Yoga Mat Premium\", \"Fitness\", \"Yoga\", 49.99, 20.00, True),\n",
    "    (\"6002\", \"Resistance Bands Set\", \"Fitness\", \"Strength\", 29.99, 12.00, True),\n",
    "    (\"6003\", \"Dumbbell Set 50lb\", \"Fitness\", \"Weights\", 149.99, 60.00, True),\n",
    "    (\"6004\", \"Kettlebell 25lb\", \"Fitness\", \"Weights\", 59.99, 24.00, True),\n",
    "    (\"6005\", \"Jump Rope Speed\", \"Fitness\", \"Cardio\", 19.99, 8.00, True),\n",
    "    (\"6006\", \"Exercise Ball 65cm\", \"Fitness\", \"Accessories\", 34.99, 14.00, True),\n",
    "    (\"6007\", \"Foam Roller\", \"Fitness\", \"Recovery\", 29.99, 12.00, True),\n",
    "    (\"6008\", \"Fitness Tracker Band\", \"Fitness\", \"Electronics\", 99.99, 40.00, True),\n",
    "    (\"7001\", \"Ski Package Adult\", \"Winter Sports\", \"Skiing\", 599.99, 240.00, True),\n",
    "    (\"7002\", \"Snowboard Complete\", \"Winter Sports\", \"Snowboarding\", 449.99, 180.00, True),\n",
    "    (\"7003\", \"Ski Goggles Pro\", \"Winter Sports\", \"Accessories\", 89.99, 36.00, True),\n",
    "    (\"7004\", \"Snow Gloves Insulated\", \"Winter Sports\", \"Apparel\", 59.99, 24.00, True),\n",
    "    (\"7005\", \"Ski Poles Aluminum\", \"Winter Sports\", \"Skiing\", 49.99, 20.00, True),\n",
    "    (\"7006\", \"Snowshoes Adult\", \"Winter Sports\", \"Snowshoeing\", 179.99, 72.00, True),\n",
    "    (\"7007\", \"Hand Warmers 10-Pack\", \"Winter Sports\", \"Accessories\", 14.99, 6.00, True),\n",
    "    (\"8001\", \"Backpack 40L\", \"Bags\", \"Hiking Packs\", 129.99, 52.00, True),\n",
    "    (\"8002\", \"Backpack 65L\", \"Bags\", \"Hiking Packs\", 179.99, 72.00, True),\n",
    "    (\"8003\", \"Daypack 25L\", \"Bags\", \"Daypacks\", 79.99, 32.00, True),\n",
    "    (\"8004\", \"Hydration Pack\", \"Bags\", \"Hydration\", 99.99, 40.00, True),\n",
    "    (\"8005\", \"Duffel Bag Large\", \"Bags\", \"Travel\", 89.99, 36.00, True),\n",
    "    (\"8006\", \"Waist Pack\", \"Bags\", \"Accessories\", 34.99, 14.00, True),\n",
    "    (\"8007\", \"Laptop Backpack\", \"Bags\", \"Urban\", 109.99, 44.00, True),\n",
    "    (\"8008\", \"Cooler Bag 24-Can\", \"Bags\", \"Coolers\", 49.99, 20.00, True),\n",
    "]\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), False),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"SubCategory\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"UnitCost\", DoubleType(), True),\n",
    "    StructField(\"IsActive\", BooleanType(), True),\n",
    "])\n",
    "\n",
    "df_products = spark.createDataFrame(products_data, products_schema)\n",
    "print(f\"Products: {df_products.count()} rows\")\n",
    "df_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc68cb6",
   "metadata": {},
   "source": [
    "## 2. Stores Table\n",
    "\n",
    "12 retail locations across Pacific Northwest and Mountain regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_data = [\n",
    "    (\"S001\", \"Downtown Flagship\", \"Seattle\", \"WA\", \"Pacific Northwest\", \"2018-03-15\", 15000, \"Sarah Mitchell\"),\n",
    "    (\"S002\", \"Eastside Mall\", \"Bellevue\", \"WA\", \"Pacific Northwest\", \"2019-06-01\", 8500, \"James Chen\"),\n",
    "    (\"S003\", \"University District\", \"Seattle\", \"WA\", \"Pacific Northwest\", \"2020-02-20\", 6000, \"Maria Rodriguez\"),\n",
    "    (\"S004\", \"Tacoma Central\", \"Tacoma\", \"WA\", \"Pacific Northwest\", \"2019-11-10\", 7500, \"David Kim\"),\n",
    "    (\"S005\", \"Portland Pearl\", \"Portland\", \"OR\", \"Pacific Northwest\", \"2018-08-22\", 12000, \"Emily Watson\"),\n",
    "    (\"S006\", \"Portland East\", \"Gresham\", \"OR\", \"Pacific Northwest\", \"2021-04-15\", 6500, \"Michael Brown\"),\n",
    "    (\"S007\", \"Eugene Campus\", \"Eugene\", \"OR\", \"Pacific Northwest\", \"2020-09-01\", 5500, \"Lisa Park\"),\n",
    "    (\"S008\", \"Boise Main\", \"Boise\", \"ID\", \"Mountain\", \"2019-03-10\", 9000, \"Robert Taylor\"),\n",
    "    (\"S009\", \"Boise West\", \"Meridian\", \"ID\", \"Mountain\", \"2022-01-20\", 5000, \"Jennifer Adams\"),\n",
    "    (\"S010\", \"Spokane Valley\", \"Spokane\", \"WA\", \"Mountain\", \"2020-06-15\", 7000, \"Chris Martinez\"),\n",
    "    (\"S011\", \"Missoula Downtown\", \"Missoula\", \"MT\", \"Mountain\", \"2021-08-01\", 5500, \"Amanda Wilson\"),\n",
    "    (\"S012\", \"Bend Outdoor\", \"Bend\", \"OR\", \"Pacific Northwest\", \"2022-05-15\", 6000, \"Kevin O'Brien\"),\n",
    "]\n",
    "\n",
    "stores_schema = StructType([\n",
    "    StructField(\"StoreID\", StringType(), False),\n",
    "    StructField(\"StoreName\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"OpenDate\", StringType(), True),\n",
    "    StructField(\"SquareFeet\", IntegerType(), True),\n",
    "    StructField(\"ManagerName\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_stores = spark.createDataFrame(stores_data, stores_schema)\n",
    "df_stores = df_stores.withColumn(\"OpenDate\", to_date(col(\"OpenDate\")))\n",
    "print(f\"Stores: {df_stores.count()} rows\")\n",
    "df_stores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738693b",
   "metadata": {},
   "source": [
    "## 3. Calendar Table\n",
    "\n",
    "Date dimension covering 2023-2025 with fiscal calendar support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0af0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, sequence, to_date, year, quarter, month, dayofweek, weekofyear\n",
    "from pyspark.sql.functions import date_format, when, lit\n",
    "\n",
    "# Generate date range\n",
    "df_calendar = spark.sql(f\"\"\"\n",
    "    SELECT explode(sequence(to_date('{START_DATE}'), to_date('{END_DATE}'), interval 1 day)) as Date\n",
    "\"\"\")\n",
    "\n",
    "# Add calendar columns\n",
    "df_calendar = df_calendar \\\n",
    "    .withColumn(\"Year\", year(\"Date\")) \\\n",
    "    .withColumn(\"Quarter\", concat(lit(\"Q\"), quarter(\"Date\"))) \\\n",
    "    .withColumn(\"Month\", month(\"Date\")) \\\n",
    "    .withColumn(\"MonthName\", date_format(\"Date\", \"MMMM\")) \\\n",
    "    .withColumn(\"WeekOfYear\", weekofyear(\"Date\")) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(\"Date\")) \\\n",
    "    .withColumn(\"DayName\", date_format(\"Date\", \"EEEE\")) \\\n",
    "    .withColumn(\"IsWeekend\", when(dayofweek(\"Date\").isin([1, 7]), True).otherwise(False)) \\\n",
    "    .withColumn(\"IsHoliday\", lit(False)) \\\n",
    "    .withColumn(\"FiscalYear\", \n",
    "        when(month(\"Date\") >= 7, concat(lit(\"FY\"), year(\"Date\") + 1))\n",
    "        .otherwise(concat(lit(\"FY\"), year(\"Date\")))) \\\n",
    "    .withColumn(\"FiscalQuarter\",\n",
    "        when(month(\"Date\").isin([7,8,9]), lit(\"FQ1\"))\n",
    "        .when(month(\"Date\").isin([10,11,12]), lit(\"FQ2\"))\n",
    "        .when(month(\"Date\").isin([1,2,3]), lit(\"FQ3\"))\n",
    "        .otherwise(lit(\"FQ4\")))\n",
    "\n",
    "# Mark US holidays (simplified)\n",
    "holidays = [\n",
    "    \"2023-01-01\", \"2023-01-16\", \"2023-05-29\", \"2023-07-04\", \"2023-09-04\", \n",
    "    \"2023-11-23\", \"2023-11-24\", \"2023-12-25\",\n",
    "    \"2024-01-01\", \"2024-01-15\", \"2024-05-27\", \"2024-07-04\", \"2024-09-02\",\n",
    "    \"2024-11-28\", \"2024-11-29\", \"2024-12-25\",\n",
    "    \"2025-01-01\", \"2025-01-20\", \"2025-05-26\", \"2025-07-04\", \"2025-09-01\",\n",
    "    \"2025-11-27\", \"2025-11-28\", \"2025-12-25\",\n",
    "]\n",
    "df_calendar = df_calendar.withColumn(\"IsHoliday\", \n",
    "    when(col(\"Date\").isin([to_date(lit(h)) for h in holidays]), True).otherwise(False))\n",
    "\n",
    "print(f\"Calendar: {df_calendar.count()} rows\")\n",
    "df_calendar.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec2ca",
   "metadata": {},
   "source": [
    "## 4. Targets Table\n",
    "\n",
    "Monthly revenue and transaction targets by store for 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build targets using Spark SQL to avoid any type/function conflicts\n",
    "\n",
    "# Create temporary view with store data\n",
    "store_targets_sql = \"\"\"\n",
    "SELECT StoreID, AnnualTarget FROM (VALUES\n",
    "    ('S001', 2700000),\n",
    "    ('S002', 1450000),\n",
    "    ('S003', 975000),\n",
    "    ('S004', 1168000),\n",
    "    ('S005', 2176000),\n",
    "    ('S006', 1028000),\n",
    "    ('S007', 907000),\n",
    "    ('S008', 1480000),\n",
    "    ('S009', 794000),\n",
    "    ('S010', 1123000),\n",
    "    ('S011', 906000),\n",
    "    ('S012', 1028000)\n",
    ") AS t(StoreID, AnnualTarget)\n",
    "\"\"\"\n",
    "\n",
    "# Create temporary view with monthly weights\n",
    "monthly_sql = \"\"\"\n",
    "SELECT MonthNum, Weight FROM (VALUES\n",
    "    (1, 0.07), (2, 0.065), (3, 0.075), (4, 0.085),\n",
    "    (5, 0.095), (6, 0.10), (7, 0.11), (8, 0.105),\n",
    "    (9, 0.09), (10, 0.08), (11, 0.10), (12, 0.12)\n",
    ") AS t(MonthNum, Weight)\n",
    "\"\"\"\n",
    "\n",
    "# Generate targets using pure SQL\n",
    "df_targets = spark.sql(f\"\"\"\n",
    "WITH stores AS ({store_targets_sql}),\n",
    "     months AS ({monthly_sql})\n",
    "SELECT \n",
    "    s.StoreID,\n",
    "    2024 AS Year,\n",
    "    m.MonthNum AS Month,\n",
    "    CAST(ROUND(s.AnnualTarget * m.Weight, -2) AS DOUBLE) AS RevenueTarget,\n",
    "    CAST(ROUND(s.AnnualTarget * m.Weight, -2) / 100 AS INT) AS TransactionTarget\n",
    "FROM stores s\n",
    "CROSS JOIN months m\n",
    "ORDER BY s.StoreID, m.MonthNum\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Targets: {df_targets.count()} rows\")\n",
    "df_targets.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0b9b1",
   "metadata": {},
   "source": [
    "## 5. Sales Transactions\n",
    "\n",
    "Generate realistic transaction data with seasonality and store weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065af601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Seasonality factors by month (outdoor retail patterns)\n",
    "SEASONALITY = {\n",
    "    1: 0.7, 2: 0.65, 3: 0.85, 4: 1.0, 5: 1.2, 6: 1.3,\n",
    "    7: 1.35, 8: 1.25, 9: 0.95, 10: 0.9, 11: 1.1, 12: 1.4\n",
    "}\n",
    "\n",
    "# Store size weights\n",
    "STORE_WEIGHTS = {\n",
    "    \"S001\": 2.0, \"S002\": 1.2, \"S003\": 0.8, \"S004\": 1.0,\n",
    "    \"S005\": 1.6, \"S006\": 0.9, \"S007\": 0.75, \"S008\": 1.3,\n",
    "    \"S009\": 0.7, \"S010\": 1.0, \"S011\": 0.75, \"S012\": 0.85\n",
    "}\n",
    "\n",
    "# Store open dates\n",
    "STORE_OPEN = {\n",
    "    \"S001\": datetime(2018, 3, 15), \"S002\": datetime(2019, 6, 1),\n",
    "    \"S003\": datetime(2020, 2, 20), \"S004\": datetime(2019, 11, 10),\n",
    "    \"S005\": datetime(2018, 8, 22), \"S006\": datetime(2021, 4, 15),\n",
    "    \"S007\": datetime(2020, 9, 1), \"S008\": datetime(2019, 3, 10),\n",
    "    \"S009\": datetime(2022, 1, 20), \"S010\": datetime(2020, 6, 15),\n",
    "    \"S011\": datetime(2021, 8, 1), \"S012\": datetime(2022, 5, 15)\n",
    "}\n",
    "\n",
    "# Collect products for local use\n",
    "products_list = df_products.collect()\n",
    "store_ids = list(STORE_WEIGHTS.keys())\n",
    "\n",
    "# Helper to safely round to 2 decimal places as float\n",
    "def round2(val):\n",
    "    return float(int(val * 100 + 0.5) / 100)\n",
    "\n",
    "def generate_transactions(num_transactions, start_id=1000001):\n",
    "    \"\"\"Generate sales transaction records\"\"\"\n",
    "    transactions = []\n",
    "    start = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
    "    total_days = (end - start).days\n",
    "    \n",
    "    for i in range(num_transactions):\n",
    "        txn_id = f\"T{start_id + i}\"\n",
    "        \n",
    "        # Random date\n",
    "        days_offset = random.randint(0, total_days)\n",
    "        txn_date = start + timedelta(days=days_offset)\n",
    "        \n",
    "        # Select store (weighted)\n",
    "        store_id = random.choices(\n",
    "            store_ids,\n",
    "            weights=[STORE_WEIGHTS[s] for s in store_ids]\n",
    "        )[0]\n",
    "        \n",
    "        # Skip if store wasn't open\n",
    "        if txn_date < STORE_OPEN[store_id]:\n",
    "            continue\n",
    "        \n",
    "        # Select product\n",
    "        product = random.choice(products_list)\n",
    "        \n",
    "        # Quantity\n",
    "        qty = 1 if random.random() < 0.8 else random.randint(2, 4)\n",
    "        \n",
    "        # Calculate amounts - explicitly cast to float\n",
    "        unit_price = float(product[\"UnitPrice\"])\n",
    "        unit_cost = float(product[\"UnitCost\"])\n",
    "        \n",
    "        # Discount (more likely in certain months)\n",
    "        if txn_date.month in [1, 2, 8, 9]:\n",
    "            discount_prob = 0.35\n",
    "        elif txn_date.month in [11, 12]:\n",
    "            discount_prob = 0.25\n",
    "        else:\n",
    "            discount_prob = 0.15\n",
    "        \n",
    "        if random.random() < discount_prob:\n",
    "            discount = round2(unit_price * random.choice([0.10, 0.15, 0.20, 0.25]))\n",
    "        else:\n",
    "            discount = 0.0\n",
    "        \n",
    "        total_amount = round2((unit_price - discount) * qty)\n",
    "        total_cost = round2(unit_cost * qty)\n",
    "        \n",
    "        # Customer ID (70% loyalty members)\n",
    "        customer_id = f\"C{random.randint(10001, 60000)}\" if random.random() < 0.7 else \"\"\n",
    "        \n",
    "        transactions.append((\n",
    "            str(txn_id),\n",
    "            str(txn_date.strftime(\"%Y-%m-%d\")),\n",
    "            str(store_id),\n",
    "            str(product[\"ProductID\"]),\n",
    "            str(customer_id),\n",
    "            int(qty),\n",
    "            float(unit_price),\n",
    "            float(discount),\n",
    "            float(total_amount),\n",
    "            float(total_cost)\n",
    "        ))\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "print(f\"Generating {NUM_TRANSACTIONS:,} transactions...\")\n",
    "sales_data = generate_transactions(NUM_TRANSACTIONS)\n",
    "print(f\"Generated {len(sales_data):,} valid transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sales DataFrame\n",
    "sales_schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), False),\n",
    "    StructField(\"TransactionDate\", StringType(), True),\n",
    "    StructField(\"StoreID\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"Discount\", DoubleType(), True),\n",
    "    StructField(\"TotalAmount\", DoubleType(), True),\n",
    "    StructField(\"TotalCost\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, schema=sales_schema)\n",
    "\n",
    "# Use SQL-style cast to avoid col() function issues\n",
    "df_sales.createOrReplaceTempView(\"sales_temp\")\n",
    "df_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        TransactionID,\n",
    "        TO_DATE(TransactionDate) AS TransactionDate,\n",
    "        StoreID,\n",
    "        ProductID,\n",
    "        CustomerID,\n",
    "        Quantity,\n",
    "        UnitPrice,\n",
    "        Discount,\n",
    "        TotalAmount,\n",
    "        TotalCost\n",
    "    FROM sales_temp\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Sales: {df_sales.count():,} rows\")\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98814e6c",
   "metadata": {},
   "source": [
    "## 6. Data Summary\n",
    "\n",
    "Quick validation of generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics using SQL to avoid function conflicts\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_txn,\n",
    "        SUM(TotalAmount) as total_revenue,\n",
    "        SUM(TotalCost) as total_cost\n",
    "    FROM sales_temp\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "total_txn = summary[\"total_txn\"]\n",
    "total_revenue = summary[\"total_revenue\"]\n",
    "total_cost = summary[\"total_cost\"]\n",
    "\n",
    "print(f\"\\nTransactions: {total_txn:,}\")\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"Total Cost: ${total_cost:,.2f}\")\n",
    "print(f\"Gross Margin: {(total_revenue - total_cost) / total_revenue * 100:.1f}%\")\n",
    "print(f\"Avg Transaction: ${total_revenue / total_txn:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"REVENUE BY YEAR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(TransactionDate) AS Year,\n",
    "        SUM(TotalAmount) AS Revenue\n",
    "    FROM sales_temp\n",
    "    GROUP BY YEAR(TransactionDate)\n",
    "    ORDER BY Year\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96817303",
   "metadata": {},
   "source": [
    "## 7. Save to Lakehouse\n",
    "\n",
    "Write all tables as Delta format to the Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mode: \"overwrite\" for fresh load, \"append\" for incremental\n",
    "mode = \"append\" if APPEND_MODE else \"overwrite\"\n",
    "\n",
    "print(f\"Saving tables (mode: {mode})...\")\n",
    "\n",
    "# Dimension tables (always overwrite)\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Products\")\n",
    "print(\"âœ“ Products\")\n",
    "\n",
    "df_stores.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Stores\")\n",
    "print(\"âœ“ Stores\")\n",
    "\n",
    "df_calendar.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Calendar\")\n",
    "print(\"âœ“ Calendar\")\n",
    "\n",
    "df_targets.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Targets\")\n",
    "print(\"âœ“ Targets\")\n",
    "\n",
    "# Fact table (can append)\n",
    "df_sales.write.format(\"delta\").mode(mode).saveAsTable(\"Sales\")\n",
    "print(f\"âœ“ Sales ({mode})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ALL TABLES SAVED TO LAKEHOUSE!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbf494",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Demo: Add Incremental Data\n",
    "\n",
    "Run this section during a demo to show data changes flowing through.\n",
    "\n",
    "**Use Case:** Show Git detecting changes after new data is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEMO: ADD NEW TRANSACTIONS ===\n",
    "# Uncomment and run during demo to add fresh data\n",
    "\n",
    "# NEW_TRANSACTIONS = 1000  # Add 1000 new transactions\n",
    "# START_ID = 2000001  # Start from a new ID range\n",
    "# \n",
    "# print(f\"Adding {NEW_TRANSACTIONS} new transactions...\")\n",
    "# new_sales_data = generate_transactions(NEW_TRANSACTIONS, start_id=START_ID)\n",
    "# df_new_sales = spark.createDataFrame(new_sales_data, sales_schema)\n",
    "# df_new_sales = df_new_sales.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\")))\n",
    "# \n",
    "# df_new_sales.write.format(\"delta\").mode(\"append\").saveAsTable(\"Sales\")\n",
    "# print(f\"âœ“ Added {df_new_sales.count()} new transactions!\")\n",
    "# \n",
    "# # Verify\n",
    "# total = spark.sql(\"SELECT COUNT(*) FROM Sales\").collect()[0][0]\n",
    "# print(f\"Total transactions now: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f974",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Create a Semantic Model** pointing to these Delta tables\n",
    "2. **Connect to Git** from the Fabric workspace\n",
    "3. **Build reports** using the Sales Analytics model\n",
    "\n",
    "See the workshop materials for detailed instructions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
